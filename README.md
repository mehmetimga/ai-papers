# AI Papers

This repository contains AI research papers and related blog posts.

## Blogs

| Title | Author | Date | Link | Notes |
|-------|--------|------|------|-------|
| Small robots, mighty vision: NASA Jet Propulsion Laboratory's DINOv2-enabled robot rovers and the future of planetary exploration | Meta | - | [Link](https://ai.meta.com/blog/nasa-jpl-dino-robot-explorers/) | DINOv2 used for autonomous navigation and terrain analysis in Mars rovers |
| How DINOv3 is helping World Resources Institute restore forests and farms globally | Meta | - | [Link](https://ai.meta.com/blog/world-resources-institute-dinov3/) | DINOv3 used for tree counting and forest restoration monitoring |
| DINOv3: Self-supervised learning for vision at unprecedented scale | Meta | - | [Link](https://ai.meta.com/blog/dinov3-self-supervised-vision-model/) | DINOv3 model trained on 1.7B images with self-supervised learning |

## Papers

| Title | Authors | Year | Link | Notes |
|-------|---------|------|------|-------|
| A Neural Probabilistic Language Model | Bengio et al. | 2003 | [PDF](papers/A%20Neural%20Probabilistic%20Language%20Model.pdf) | Foundational paper introducing neural language models with distributed word representations |
| A comprehensive taxonomy of hallucinations in Large Language Models | Cossio | 2024 | [PDF](papers/A%20comprehensive%20taxonomy%20of%20hallucinations%20in%20Large%20Language%20Models%20.pdf) | Formal definition and theoretical framework for LLM hallucinations |
| AN IMAGE IS WORTH 16X16 WORDS | Dosovitskiy et al. | 2021 | [PDF](papers/AN%20IMAGE%20IS%20WORTH%2016X16%20WORDS.pdf) | Vision Transformer (ViT) - applies transformers to image classification |
| Attention Is All You Need | Vaswani et al. | 2017 | [PDF](papers/Attention%20Is%20All%20You%20Need.pdf) | Introduced the Transformer architecture, foundational for modern AI |
| DINOv2: Learning Robust Visual Features without Supervision | Oquab et al. | 2023 | [PDF](papers/DINOv2-%20Learning%20Robust%20Visual%20Features%20without%20Supervision.pdf) | Self-supervised learning method for creating foundation models in computer vision |
| Deep Learning Advancements in Anomaly Detection: A Comprehensive Survey | Huang et al. | 2025 | [PDF](papers/Deep%20Learning%20Advancements%20in%20Anomaly%20Detection-%20A%20Comprehensive%20Survey.pdf) | Review of 180+ studies on deep learning-based anomaly detection |
| Deep Neural Networks for YouTube Recommendations | Covington et al. | 2016 | [PDF](papers/Deep%20Neural%20Networks%20for%20YouTube%20Recommendations.pdf) | Two-stage deep learning architecture for large-scale recommendation systems |
| Generative Medical Event Models Improve with Scale | Waxler et al. | 2024 | [PDF](papers/Generative%20Medical%20Event%20Models%20Improve%20with%20Scale.pdf) | CoMET models trained on 118M patients and 115B medical events |
| Graph Convolutional Neural Networks for Web-Scale Recommender Systems | Ying et al. | 2018 | [PDF](papers/Graph%20Convolutional%20Neural%20Networks%20for%20Web-Scale%20Recommender%20Systems.pdf) | PinSage - Graph neural network deployed at Pinterest scale |
| Language Models are Few-Shot Learners | Brown et al. | 2020 | [PDF](papers/Language%20Models%20are%20Few-Shot%20Learners.pdf) | GPT-3 paper - 175B parameter model demonstrating few-shot learning |
| NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE | Bahdanau et al. | 2015 | [PDF](papers/NEURAL%20MACHINE%20TRANSLATION%20BY%20JOINTLY%20LEARNING%20TO%20ALIGN%20AND%20TRANSLATE.pdf) | First attention mechanism paper for neural machine translation |
| Parallax: Visualizing and Understanding the Semantics of Embedding Spaces via Algebraic Formulae | Molino et al. | 2019 | [PDF](papers/Parallax-%20Visualizing%20and%20Understanding%20the%20Semantics%20of%20Embedding%20Spaces%20via%20Algebraic%20Formulae.pdf) | Tool for interpretable embedding visualization using algebraic formulae |
| STEAM RECOMMENDATION SYSTEM | Batra et al. | 2023 | [PDF](papers/STEAM%20RECOMMENDATION%20SYSTEM.pdf) | Game recommendation system for Steam platform using Apache Spark |
| Sequence to Sequence Learning with Neural Networks | Sutskever et al. | 2014 | [PDF](papers/Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.pdf) | Introduced seq2seq architecture using LSTMs for sequence-to-sequence tasks |
| Transformers Learn In-Context by Gradient Descent | von Oswald et al. | 2023 | [PDF](papers/Transformers%20Learn%20In-Context%20by%20Gradient%20Descent.pdf) | Shows equivalence between transformer attention and gradient descent |
| WHAT LEARNING ALGORITHM IS IN-CONTEXT LEARNING? INVESTIGATIONS WITH LINEAR MODELS | Akyürek et al. | 2023 | [PDF](papers/WHAT%20LEARNING%20ALGORITHM%20IS%20IN-CONTEXT%20LEARNING%3F%20INVESTIGATIONS%20WITH%20LINEAR%20MODELS.pdf) | Investigates the learning mechanisms behind in-context learning |
| Multi-variable continuous time generalised predictive control | Demircioǧlu & Gawthrop | 1992 | [PDF](papers/multi-variable-continuous-time-generalised-predictive-control.pdf) | MCGPC - Multivariable extension of continuous-time generalized predictive control. Significant contribution to Model Predictive Control (MPC) literature, establishing conditions for decoupling and model-following, and showing the relationship with LQ control |
| DINOv3 | Siméoni et al. | 2025 | [PDF](papers/DINOv3.pdf) | Self-supervised vision model trained on 1.7B images with 7B parameters |
| Disentangling the Factors of Convergence between Brains and Computer Vision Models | Raugel et al. | 2024 | [PDF](papers/Disentangling%20the%20Factors%20of%20Convergence%20between%20Brains%20and%20Computer%20Vision%20Models.pdf) | Studies how AI vision models develop brain-like representations |
| Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions | Tang et al. | 2025 | [PDF](papers/Efficient%20Speculative%20Decoding%20for%20Llama%20at%20Scale-%20Challenges%20and%20Solutions.pdf) | Production-scale EAGLE-based speculative decoding for Llama models |
| FastCSP: Accelerated Molecular Crystal Structure Prediction with Universal Model for Atoms | Gharakhanyan et al. | 2024 | [PDF](papers/FastCSP-%20Accelerated%20Molecular%20Crystal%20Structure%20Prediction%20with%20Universal%20Model%20for%20Atoms.pdf) | Open-source CSP workflow using machine learning interatomic potentials |
| Open Molecular Crystals 2025 (OMC25) Dataset and Models | Gharakhanyan et al. | 2025 | [PDF](papers/Open%20Molecular%20Crystals%202025%20(OMC25)%20Dataset%20and%20Models.pdf) | Dataset of 27M+ molecular crystal structures from DFT calculations |